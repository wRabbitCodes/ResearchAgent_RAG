# === LLM & Memory Settings ===

# Number of past conversational turns to retain in memory.
# Type: int
MAX_MEMORY_TURNS=5

# Backend to use for language model inference.
# Type: str
# Options: OLLAMA_BACKEND, LLM_CPP_BACKEND
LLM_BACKEND=OLLAMA_BACKEND

# Name of the ChromaDB collection to store documents.
# Type: str
CHROMADB_NAME=documents

# Base URL for the Ollama API endpoint (only used if OLLAMA_BACKEND is selected).
# Type: str (URL)
OLLAMA_CLIENT_URL=http://localhost:11434

# Name of the embedding model used to generate document/query vectors.
# Type: str
# Examples: all-MiniLM-L6-v2-onnx
EMBEDDING_MODEL=all-MiniLM-L6-v2-onnx

# Maximum number of tokens allowed per LLM generation request.
# Type: int
MAX_TOKEN=512

# Total token capacity of the LLMâ€™s context window (prompt + memory + response).
# Type: int
CONTEXT_WINDOW=4096

# === Tooling & Behavior Flags ===

# Enable or disable the use of external tools like Wikipedia, calculators, etc.
# Type: bool (true/false)
USE_TOOL=false

# Minimum confidence required to trigger a tool call.
# Type: float (0.0 to 1.0)
TOOLING_THRESHOLD=0.3

# === LLaMA.cpp Backend Specific ===

# Number of transformer layers to offload to GPU (for llama.cpp backend only).
# Type: int
# Set to 0 to run entirely on CPU.
LLAMA_CPP_GPU_LAYERS=0

# === Request Config ===

# Maximum time (in seconds) to wait for a response from the LLM backend.
# Type: int
REQUEST_TIMEOUT=30
